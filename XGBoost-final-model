# Conda
# conda install -c conda-forge xgboost
# Import necessary packages
# XGBoost 
import xgboost as xgb
from xgboost import plot_importance
# Pandas
import pandas as pd 
from pandas import read_csv
from pandas import DataFrame
from pandas import concat
# SKLearn
from sklearn.model_selection import TimeSeriesSplit, GridSearchCV, train_test_split
from sklearn.metrics import make_scorer, mean_absolute_percentage_error, mean_absolute_error, r2_score, mean_squared_error
from sklearn.feature_selection import SelectFromModel
# Plotting
from matplotlib import pyplot as plt
# Datetime
import datetime
from datetime import datetime
# Numpy
import numpy as np
from numpy import sort
# Timer 
import time

# After feature selection
# Drop bottom 6 features and refit, predict
series = read_csv('dmatrix-reducedVAR-omitNA-wswdUV.csv', header=0, index_col=0)

series = series.drop(["PM25(t-24m)","PM10(t-24m)","NOx(t-1)",
                      "SO2(t-24m)","NO2(t-6)"], axis=1)

# Train
train = series.iloc[:5241,:]
x_train = train.iloc[:,:43]
y_train = train.iloc[:,43:]

# Test
test = series.iloc[5241:,:]
x_test = test.iloc[:,:43]
y_test = test.iloc[:,43:]
# Evaluation set for final model
eval_set = [(x_test, y_test)]

# Update model
# Add early stopping round to avoid 
model = xgb.XGBRegressor(objective='reg:squarederror', 
                         eta = 0.1, 
                         max_depth = 8, 
                         min_child_weight = 2.5, 
                         n_estimators = 100,
                         gamma = 0.1,
                         early_stopping_round = 10)

model.fit(x_train, y_train, eval_set=eval_set)     # fit
y_pred = model.predict(x_test)  # predict
